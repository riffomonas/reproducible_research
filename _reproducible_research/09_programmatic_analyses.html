---
layout: presentation
title: Programmatic analyses
permalink: /programmatic_analyses/
---
class: middle center

# {{ page.title }}

.footnote.left.gray[Press 'h' to open the help menu for interacting with the slides]

---
## Learning goals
* Articulate the strengths and weaknesses of point-and-click vs. command line tools
* Develop a set of best practices for scripting the analysis of data in a programmatic manner
* Define and identify violations of the DRY principle
* Apply tools that will make analyses reproducible when using random data
* Use tools to protect yourself and others from weird data

---

## Case study
* You generate a visualization of the NMDS ordination data we generated in the last tutorial using a point-and-click package (e.g. Excel, Prism). You mimic the boring shape and coloring scheme that was in the original Kozich et al. paper. You show your PI, which gets them excited
--

* Your PI starts asking questions...
  * Can you change the color and shape of the plotting symbols?
	* Is there a way to plot this in 3D with 3 NMDS axes?
	* Is there a way to plot this in 3D with 2 NMDS axes and time?
	* Could you create an animation over time?

???

What do you think?

---

## Point-and-click has significant drawbacks

* Expense
* Centralized development
* Cannot be automated
* Not flexible

---

## Open Source: Programmatic Analyses

* Tend to be "open source"
* Free, decentralized, automatable, (overly) flexible
* Extendible

---

## Changing aesthetics

* Change an option value, rerun the script
* Entire hexidecimal color pallete

---

class: center middle

![Wes Anderson collor pallette]({{ site.baseurl }}/assets/images/wesanderson.png)

.left.footnote[[wesanderson package](https://github.com/karthik/wesanderson)]
---

class: center middle

![Beyonce collor pallette]({{ site.baseurl }}/assets/images/beyonce.png)

.left.footnote[[beyonce package](https://github.com/dill/beyonce)]
---

class: center middle

![catterplots R package]({{ site.baseurl }}/assets/images/catterplots.png)

.left.footnote[[catterplots package](https://github.com/Gibbsdavidl/CatterPlots)]
---

## Think *different*

* Numerous packages in R, Python, etc for data visualization
* Interactive plots and analysis
* All generated by distributed groups of people that had a need, solved the problem, and made is available to others to use or improve upon

---

class: center middle

![Animated version of NMDS]({{ site.baseurl }}/assets/images/animated-nmds.gif)

.left.footnote[[RGL package](https://cran.r-project.org/web/packages/rgl/index.html)]

---

class: center middle

![Animated 2D Scatter plot]({{ site.baseurl }}/assets/images/gganimate.gif)

.left.footnote[[gganimate package](https://github.com/dgrtwo/gganimate)]

---

class: middle center

## .alert[That's fancy and all...<br><br>... but it's eye candy. Don't lose track of the ablity to make analyses more reproducibe because we can ***programmatically*** generate these plots]

---

## Interactive vs. programmatic analyses

* You can run R and other scripting languages in an interactive mode
* This has many of the same limitations of point-and-click tools
* The difference between an interactive and programmatic analysis is that the programmatic analysis utilizes a script to store the commands for repeated and automated use

---

## Programming

* We've already made a couple of scripts in `bash` and `mothur`
* Scripts are text-based files for automating analyses
* These are the way we tell the computer how to make a paper airplane (or something more useful)
* There are many formal languages: (bash), R, Python, Perl, Julia, Go, Java, C/C++, Fortran, Pascal, etc

???

We're going to assume that you know enough bash to get by and will slowly develop these skills as you work in the command line more and more

---

## Which language should you chose?

* Dunno. It depends.
--

* Narrow the list
  * R
  * Python
* The others are either significantly harder to use, aren't widely used, or aren't fully developed yet
--

* Start asking questions
--

  * What do your labmates use?
--

  * What do others on campus use?
--

  * If you needed to solve a problem or have someone check your code, who would you ask? What do they use?

---

class: middle center

## .alert[Find your community and nourish it]

![It's about your community and what they are using]({{ site.baseurl }}/assets/images/teamwork.gif)

???

---

class: middle center

## .alert[Beware brogrammers and language wars]

![Avoid language wars at all cost]({{ site.baseurl }}/assets/images/brogrammer.gif)

???

* There's a lot of "brogrammers" out there that will likely tell you what you *should* be learning. Run. Fast.
* Most of the differences between languages are lost on all but the most proficient programmers
---

## R

* My lab, many people at my institution, and I use R
* I really like the broader R community
* Won't try to teach you R here, there are numerous other resources for that (see my [minimalR tutorial](http://www.riffomonas.org/minimalR/))
* Pretty much everything I say about R is true for Python
* The goal is to demonstrate how to programmatically analyze data - the tooling doesn't matter

---

## General principles

* Worry about the outcome, not the path
* Don't Repeat Yourself (DRY)
* Defensive programming and testing
* Setting random number generator seed
* Package versions
* R quirks to avoid

---

## Worry about the outcome, not the path

* The most important thing is to get the correct answer
* Don't worry so much about whether you wrote the most efficient program
* If your analysis is taking a long time to run, then worry about your efficiency
* There are many [tricks and tools](https://datascienceplus.com/strategies-to-speedup-r-code/) for speeding up R (again, beware language wars)

---

## Don't Repeat Yourself (DRY)

* If you have a line(s) of code that you use multiple times within a script or across scripts, put that into a function or define a variable
* You can replace those lines where there was repitition with a call to the function or variable
* Go from maintaining 5 copies of code to 1
* Significantly limits headaches

---

## Examples...

* Define color and symbol scheme

```R
figure_colors <- c(early="red", late="blue")
figure_pchs <- c(early=19, late=19)
```

* Utility functions

```R
calc_shannon <- function(otu_count_vector){
	relative_abundance <- otu_count_vector / sum(otu_count_vector)
	shannon <- -1 * relative_abundance * sum(log(relative_abundance))
	return(shannon)
}
```
---

## Doh.

```R
calc_shannon <- function(otu_count_vector){
	relative_abundance <- otu_count_vector / sum(otu_count_vector)
	shannon <- -1 * relative_abundance * sum(log(relative_abundance))
	return(shannon)
}
```

Let's try that again...

```R
calc_shannon <- function(otu_count_vector){
	relative_abundance <- otu_count_vector / sum(otu_count_vector)
	shannon <- -1 * sum(relative_abundance * log(relative_abundance))
	return(shannon)
}
```

--
<br>
If I want to change a color or plotting symbol or correct a bug in the calculation of the Shannon index or make it more efficient, I only need to change these lines of code - not where they're repeated

---

## As your analysis grows...

* As an example, assume that you have three figures and you want to use the same coloring and symbol scheme for each. To make things compartamentalized, you put the code for each figure in a separate R script.
--

* You'd have to define `figure_colors` and `figure_pchs` in each R script.
--

<br>

.center.alert[Not DRY.]

---

## Create a `code/utilities.R` file

<br>
```R
#code/utilities.R
figure_colors <- c(early="red", late="blue")
figure_pchs <- c(early=19, late=19)
```
--
<br>
Then you can run this in each of your scripts:

```R
source('code/utilities.R')
```

--
<br>
.alert.center[Good practice to put this source command and any library loads at the top of the script]
---

## "Wow, that's a useful set of functions!"

* Your labmates might really like some functions you've written
--

* Your collaborators might really like some functions you've written
--

* Some unknown person on the otherside of the world might really like some functions you've written
--

* [Write a package](http://r-pkgs.had.co.nz) - Some have suggested that one way to encapsulate a data analysis is to [make a package out of it](https://github.com/ropensci/rrrpkg)
---

## Defensive programming and testing

.left-column[
* You are your own worst enemy
* Users do weird things
* Try to anticipate weird things...
  * Providing a function the wrong type of variable
  * Dividing by zero
* Build in code to test for weird behavior and either fail nicely or give an error message
]

.right-column[
.right[![Problem exists between keyboard and chair meme]({{site.baseurl}}/assets/images/pebkac.jpg)
]]

---

## Defensive programming and testing: Example

```R
calc_shannon <- function(otu_count_vector){
	relative_abundance <- otu_count_vector / sum(otu_count_vector)
	shannon <- -1 * sum(relative_abundance * log(relative_abundance))
	return(shannon)
}
data <- c(-1,-2,-3,-4)
calc_shannon(data)
#[1] 1.279854
```

Hmmm. That's not good.

--

<p style="font-size:20px"><br><p>

```R
calc_shannon <- function(otu_count_vector){
	if(all(otu_count_vector >= 0)){
		relative_abundance <- otu_count_vector / sum(otu_count_vector)
		shannon <- -1 * sum(relative_abundance * log(relative_abundance))
	} else {
		warning("One or more of the values were less than zero.")
		shannon <- NA
	}
	return(shannon)
}
data <- c(-1,-2,-3,-4)
calc_shannon(data)
#[1] NA
#Warning message:
#In calc_shannon(data) : One or more of the values were less than zero.
```
Better.

---

## Defensive programming and testing: Tools

* `if() ... else if() ... else` - the final `else` is a catch all for situations you can't anticipate
* `length` and `is.xxxxx` functions (e.g. `is.numeric`, `is.character`, `is.logical`)
* `all` and `any`
* `stopifnot` - script stops if something is not true
* Test code with known examples
* [`testthat` package](https://github.com/hadley/testthat) for automating code tests

---

## Setting random number generator seed

* Some functions make use of random numbers to perform an analysis
* Making use of the ability to set the random number generator seed will insure that you get the same randomness each time
* Remember `get_shared_otus.batch`?

```bash
set.dir(input=data/mothur, output=data/mothur, seed=19760620)
```
* This set the mothur random number generator to `19760620` (my birthday in [ISO 8601 format](https://en.wikipedia.org/wiki/ISO_8601))

---

## Setting random number generator seed in R

```R
> runif(5)
[1] 0.8870255 0.8145864 0.7129467 0.6600283 0.1066385
> runif(5)
[1] 0.1168198 0.5150428 0.1028263 0.9711794 0.7676322
> runif(5)
[1] 0.05080043 0.51089731 0.99899027 0.84039403 0.08163530

> set.seed(19760620)
> runif(5)
[1] 0.96847218 0.11941491 0.42317798 0.09717774 0.94767339
> set.seed(19760620)
> runif(5)
[1] 0.96847218 0.11941491 0.42317798 0.09717774 0.94767339
> set.seed(19760620)
> runif(5)
[1] 0.96847218 0.11941491 0.42317798 0.09717774 0.94767339
```

* The general gist of your analysis should be insensitive to the random seed
* Pick a number (e.g. birthday, anniversary, 1) and stick to it across your analyses

---

## Package versions

* Packages change either through incroporating or deprecating options or changing underlying algorithms
* Need to know version numbers of code being used so that others (you!) can replicate your work or understand why there are differences

---

## Tooling

The output from `sessionInfo` would ideally go into the general `README` file

```R
> sessionInfo()
R version 3.3.2 (2016-10-31)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X El Capitan 10.11.6

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] knitr_1.15.10 rmarkdown_1.3

loaded via a namespace (and not attached):
 [1] backports_1.0.5 magrittr_1.5    rprojroot_1.2   tools_3.3.2
 [5] htmltools_0.3.5 Rcpp_0.12.8     stringi_1.1.2   stringr_1.2.0
 [9] digest_0.6.12   evaluate_0.10
```

Or for specific packages

```R
> packageDescription("dplyr")$Version
[1] "0.5.0"
```

---

## Tooling for package versions

* Install old versions of packages using the [`devtools` package](https://github.com/hadley/devtools)

```R
library(devtools)
install_version("ggplot2", version = "0.9.1", repos = "http://cran.us.r-project.org")
```
--

* [`packrat` package](https://rstudio.github.io/packrat/): Installs specific versions of packages in an isolated manner that can be transferred between people
--

* A bunch of complex systems that only demonstrate how hard software versioning is (e.g. [Docker](https://www.docker.com))
---

## R quirks to appreciate for effects on reproducibility

* `setwd` - NEVER
* `attach` - NEVER
* `.Rprofile` - If you have one, it must accompany project and don't put much/anything in `~/.Rprofile`
* Do not save on quitting:

```R
R --no-save --no-restore-data
```

???
I have this line in my `~/.bashrc` file:

`alias R='R --no-save --no-restore-data'`


---

## Ways to leverage R tooling

* Interact with files from other software (e.g. Excel)
* Checking metadata formatting
* Plotting

---

## Remember that raw data should stay raw

* Variety of `read.xxxxx` commands and packages for importing data from other formats
* [`openxlsx` package](https://cran.r-project.org/web/packages/openxlsx/openxlsx.pdf) for reading and writing Excel files
* [`haven` package](https://github.com/tidyverse/haven) for reading in SAS, SPSS, and Stata data files
* [`googlesheets` packge](https://github.com/jennybc/googlesheets) for accessing and managing Google spreadsheets
* Variety of tools and APIs for accessing web content (e.g. [`httr`](https://github.com/hadley/httr), [`rvest`](https://github.com/hadley/rvest), [`rentrez`](https://github.com/ropensci/rentrez))

---

class: middle center

[![Other people's data]({{ site.baseurl }}/assets/images/otherpeoplesdata.png)](https://twitter.com/cbahlai/status/649285357016584192)

---

## Checking metadata formatting

* `summary(data$crop)`
  - Check for data type (e.g. numeric, logic, character)
  - Check for range (e.g. weights of zero)
  - Check for NA values
* `table(data$crop)`
  - Check for number of different types and their frequency
* `gsub('corn ', 'corn', data$crop)`
* [`validate` package](https://cran.r-project.org/web/packages/validate/vignettes/intro.html) allows you to `check_that` variables meet certain criteria
* [googleforms](https://www.google.com/forms/about/) for controlled data entry

---

## Running R for programmatic analysis

From within R

```R
> source('code/plot_nmds.R')
```
--

<br>
From command line

```R
$ R -e "source('code/plot_nmds.R')"
```
--

<br>
Another example

```R
$ R -e "source('code/utilities.R'); calc_shannon(c(1,1,1,1,2,4,5,6,7))"
```

---

class: middle center

![Too much talking, let's do something ]({{ site.baseurl }}/assets/images/a_lot_of_talking.gif)

???

* Too much talking, let's do something!
* Again, this wasn't meant to be a how to program in R tutorial
* I wanted to highlight the tooling and approaches you might take in R to make your analysis more reproducible
---

class: middle

```R
################################################################################
#
# plot_nmds.R
#
# Here we take in the *.nmds.axes file from the mouse stability analysis and
# plot it in R as we did in Figure 4 of Kozich et al.
#
# Dependencies:   2-D axes file generated by the nmds command in mothur
# Produces:       results/figures/nmds_figure.png
#
################################################################################

plot_nmds <- function(axes_file){
  axes <- read.table(file=axes_file, header=T, row.names=1)
  day <- as.numeric(gsub(".*D(\\d*)$", "\\1", rownames(axes)))
  early <- day <= 10
  late <- day >= 140 & day <= 150

  plot_axes <- axes[early | late, ]
  plot_day <- day[early | late]
  plot_early <- early[early | late]
  plot_late <- late[early | late]

  pch <- vector()
  pch[plot_early] <- 21
  pch[plot_late] <- 19

  output_file_name <- "results/figures/nmds_figure.png"

  png(file=output_file_name)
    plot(plot_axes$axis2~plot_axes$axis1, pch=pch, xlab="PCoA Axis 1",
					ylab="PCoA Axis 2")
    legend(x=max(plot_axes$axis1)-0.125, y=min(plot_axes$axis2)+0.125,
					legend=c("Early", "Late"), pch=c(21,19))
  dev.off()
}
```
???

* Note:
  - the use of comments here is pretty spartan - but you were to add comments in the documentation tutorial
  - I changed the output file name
  - I changed the variable names to snake case with underscores
  - there isn't much defensive programming going on here

---

## Exercise

* Add a line to the `analysis_driver.bash` script to run the R file from the command line
* Run the code and commit the new file and changes
* Add comments to `code/plot_nmds.R` and add one or two things to make the code more defensive (e.g. confirm number of columns is 2; what if the names don't have a "D"?)
* Run the code and commit the new file and changes
